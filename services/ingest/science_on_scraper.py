# -*- coding: utf-8 -*-
import hashlib
import re
import sys
import json
from datetime import datetime
from urllib.parse import urlparse, urljoin

import requests
from requests.adapters import HTTPAdapter, Retry
from bs4 import BeautifulSoup, Tag

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
DEFAULT_SOURCE = "scienceON"

# --- ì¹´í…Œê³ ë¦¬ ê·œì¹™ (source ê¸°ë³¸ê°’ + ì œëª© í‚¤ì›Œë“œ ë®ì–´ì“°ê¸°) ---
CATEGORY_DEFAULT = "ê³¼í•™ê¸°ìˆ "
CATEGORY_RULES = [
    (re.compile(r"(AI|ì¸ê³µì§€ëŠ¥|GPT|LLM|ë”¥ëŸ¬ë‹)", re.I), "AI"),
    (re.compile(r"(ë³´ì•ˆ|í•´í‚¹|ìœ ì¶œ|ì·¨ì•½|ëœì„¬)", re.I), "ë³´ì•ˆ"),
    (re.compile(r"(ë°˜ë„ì²´|ì¹©|íŒŒìš´ë“œë¦¬)", re.I), "ë°˜ë„ì²´"),
    (re.compile(r"(ë¡œë´‡|ë¡œë³´í‹±ìŠ¤)", re.I), "ë¡œë´‡"),
    (re.compile(r"(ìƒëª…|ì œì•½|ë°±ì‹ |ìœ ì „ì²´|ë¯¸ìƒë¬¼|ë°”ì´ì˜¤|ì‹ ì•½|í•­ì•”ì œ)", re.I), "ìƒëª…ê³¼í•™"),
]

def categorize(title: str, fallback: str | None = None) -> str:
    base = fallback or CATEGORY_DEFAULT
    if not title:
        return base
    for pattern, label in CATEGORY_RULES:
        if pattern.search(title):
            return label
    return base

session = requests.Session()
retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])
session.mount("https://", HTTPAdapter(max_retries=retries))
session.mount("http://", HTTPAdapter(max_retries=retries))
session.headers.update(HEADERS)
BASE_DETAIL = "https://scienceon.kisti.re.kr/trendPromo/PORTrendPromoDtl.do?trendPromoNo={no}"

# ---------- text utils ----------
def clean_text(s: str) -> str:
    if not s: return ""
    s = s.replace("\xa0", " ").replace("\u00a0", " ")
    s = re.sub(r"\s+", " ", s)
    return s.strip()


def sha1_hex(value: str) -> str:
    return hashlib.sha1(value.encode("utf-8")).hexdigest()


def truncate_summary(text: str, max_sentences: int = 3, max_chars: int = 180) -> str:
    if not text:
        return ""
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸° (í•œê¸€ ë§ˆì¹¨í‘œ í¬í•¨)
    normalized = text.replace("\n", " ")
    pieces = re.split(r"(?<=[.!?\u3002\uFF01\uFF1F])\s+", normalized)
    sentences = []
    for piece in pieces:
        part = clean_text(piece)
        if part:
            sentences.append(part)
        if len(sentences) >= max_sentences:
            break
    if not sentences:
        sentences = [clean_text(text)]
    summary = " ".join(sentences)
    if len(summary) > max_chars:
        summary = summary[: max_chars - 1].rstrip() + "â€¦"
    return summary

def looks_like_title(t: str) -> bool:
    if not t or len(t) < 4: return False
    if t.lower() in {"new", "(new)"}: return False
    return True

def smart_title_clean(s: str) -> str:
    # ë¶ˆë¦¿ ì´í›„/ë‚ ì§œ ê¼¬ë¦¬í‘œ ì œê±°
    s = clean_text(s)
    if "ã†" in s: s = s.split("ã†", 1)[0].strip()
    s = re.sub(r"\(\s*[^()]*\d{2,4}[.\-/]\d{1,2}([.\-/]\d{1,2})?[^()]*\)\s*$", "", s).strip()
    if len(s) > 180: s = s[:180].rstrip()
    return s

def strip_anchor_text(node: Tag) -> str:
    soup = BeautifulSoup(str(node), "lxml")
    for a in soup.find_all("a"): a.decompose()
    return soup.get_text(" ", strip=True)

def has_korean(t: str) -> bool:
    return bool(re.search(r"[ê°€-í£]", t or ""))

# ---------- page structure helpers ----------
def is_title_paragraph(p: Tag) -> bool:
    return (p.name == "p" and "MsoNormal" in (p.get("class") or []) and p.find("a", href=True) is not None)

def extract_link_from_p(p: Tag) -> str:
    for a in p.find_all("a", href=True):
        href = (a.get("href") or "").strip()
        if href.startswith("http"): return href
    return ""

def extract_title_from_p(p: Tag) -> str:
    # aê°€ ë“¤ì–´ìˆëŠ” ê°€ì¥ ê°€ê¹Œìš´ ì»¨í…Œì´ë„ˆ(span/strong/em) ìš°ì„ 
    a = p.find("a", href=True)
    if a:
        cur, depth = a.parent, 0
        while isinstance(cur, Tag) and depth < 5:
            if cur.name in {"span", "strong", "em"}:
                t = smart_title_clean(strip_anchor_text(cur))
                if looks_like_title(t): return t
            if cur == p: break
            cur = cur.parent; depth += 1
    # pì—ì„œ a ì œê±°
    t2 = smart_title_clean(strip_anchor_text(p))
    if looks_like_title(t2): return t2
    # ìµœí›„: p ì „ì²´
    title = smart_title_clean(p.get_text(" ", strip=True))
    # ì¹´í…Œê³ ë¦¬ í”„ë¦¬í”½ìŠ¤ ì œê±°
    title = strip_prefix_category(title)
    return title

def strip_prefix_category(title: str) -> str:
    """
    ì œëª© ë§¨ ì•ì˜ '- ( ... )' ë˜ëŠ” '( ... )' íŒ¨í„´ì„ ì œê±°.
    ì˜ˆ: '- ( ğŸ’¥ ì–‘ì ) ê±°ì‹œì„¸ê³„...' â†’ 'ê±°ì‹œì„¸ê³„...'
    """
    # ë§¨ ì•ì˜ ëŒ€ì‹œì™€ ê´„í˜¸ ë¸”ë¡ ì œê±°
    title = re.sub(r"^\s*-\s*\([^)]*\)\s*", "", title)
    title = re.sub(r"^\s*\([^)]*\)\s*", "", title)
    return title.strip()


# ---------- summary finder ----------
BULLET_START = re.compile(r"^[\s\u00A0]*[ã†Â·â€¢\-]+[\s\u00A0]*")

def split_first_bullet_line(text: str) -> str:
    """
    ê¸´ ë¬¸ë‹¨ì— ë¶ˆë¦¿ì´ ì—¬ëŸ¬ ê°œ ì´ì–´ì§ˆ ë•Œ, 'ì²« ë¶ˆë¦¿'ë§Œ í•œ ì¤„ ìš”ì•½ìœ¼ë¡œ.
    ì˜ˆ: 'ã† ... - ã† ...' í˜•íƒœ â†’ ì²« ë¶ˆë¦¿ ì¡°ê°ë§Œ.
    """
    # ë¨¼ì € ë§¨ ì• ë¶ˆë¦¿ ì œê±°
    t = BULLET_START.sub("", text).strip()
    # ë‹¤ìŒ ë¶ˆë¦¿/ëŒ€ì‹œ ì•ê¹Œì§€ ìë¥´ê¸°
    t = re.split(r"\s(?:[ã†Â·â€¢\-]\s)", t, maxsplit=1)[0]
    return clean_text(t)

def find_summary_between(root: Tag, start: Tag, end: Tag | None) -> str:
    """
    start(ì œëª© p) ì´í›„ë¶€í„° end(ë‹¤ìŒ ì œëª© p) ì´ì „ê¹Œì§€ ë²”ìœ„ë¥¼ í›‘ì–´ ìš”ì•½ í›„ë³´ë¥¼ ì°¾ëŠ”ë‹¤.
    ìš°ì„ ìˆœìœ„:
      1) ë¶ˆë¦¿ í¬í•¨ p (í•œê¸€ ìš°ì„ )
      2) ë§í¬ ì—†ëŠ” 14px ìŠ¤íƒ€ì¼ p (í•œê¸€ ìš°ì„ )
      3) ë§í¬ ì—†ëŠ” ì¼ë°˜ p (í•œê¸€ ìš°ì„ , 10~300ì)
    ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
    """
    # í›„ë³´ ë²„í‚·
    bullet_ko = None; bullet_any = None
    body14_ko = None; body14_any = None
    body_ko = None; body_any = None

    # next_elementsë¥¼ ì¨ì„œ h2 ê²½ê³„ë„ í†µê³¼
    for el in start.next_elements:
        if el is end: break
        if not isinstance(el, Tag): continue

        # ë‹¤ìŒ ì œëª© pë¥¼ ë§Œë‚˜ë©´ ì¢…ë£Œ
        if is_title_paragraph(el) and el is not start:
            break

        if el.name == "p" and "MsoNormal" in (el.get("class") or []):
            has_link = el.find("a", href=True) is not None
            raw = el.get_text(" ", strip=True)
            txt = clean_text(raw)

            # 1) ë¶ˆë¦¿ ì¼€ì´ìŠ¤
            if BULLET_START.search(txt):
                one = split_first_bullet_line(txt)
                if has_korean(one) and not bullet_ko: bullet_ko = one
                if not bullet_any: bullet_any = one
                continue

            # 2) 14px ë³¸ë¬¸ & ë§í¬ ì—†ìŒ
            if not has_link and "font-size: 14px" in (el.get("style") or ""):
                if has_korean(txt) and not body14_ko: body14_ko = txt
                if not body14_any: body14_any = txt
                continue

            # 3) ì¼ë°˜ ë³¸ë¬¸ & ë§í¬ ì—†ìŒ (ë„ˆë¬´ ê¸´ ê±´ ì œì™¸)
            if not has_link and 10 <= len(txt) <= 300:
                if has_korean(txt) and not body_ko: body_ko = txt
                if not body_any: body_any = txt

    return bullet_ko or body14_ko or body_ko or bullet_any or body14_any or body_any or ""

# ---------- optional external fallback ----------
def fetch_fallback_summary_from_link(link: str) -> str:
    try:
        rr = session.get(link, timeout=10)
        rr.raise_for_status()
    except Exception:
        return ""
    if not rr.encoding or rr.encoding.lower() in ("iso-8859-1", "us-ascii"):
        rr.encoding = rr.apparent_encoding
    ss = BeautifulSoup(rr.text, "lxml")
    for sel in ["meta[property='og:description'][content]", "meta[name='description'][content]"]:
        m = ss.select_one(sel)
        if m and m.get("content"):
            return clean_text(m["content"])
    return ""

# ---------- image helpers (NEW) ----------

FIRST_IMAGE_CACHE: dict[str, str] = {}

def _parse_srcset_best(srcset: str) -> str:
    """
    srcsetì—ì„œ ê°€ì¥ í° wë¥¼ ê°€ì§„ ì´ë¯¸ì§€ URL ë°˜í™˜. ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´.
    """
    best_url, best_w = "", -1
    for part in srcset.split(","):
        chunk = part.strip()
        if not chunk:
            continue
        # "url 777w" í˜•íƒœ
        m = re.match(r"(.+?)\s+(\d+)w", chunk)
        if m:
            url = m.group(1).strip()
            try:
                w = int(m.group(2))
            except ValueError:
                w = -1
        else:
            # w í‘œê¸°ê°€ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ê²ƒì„ ìš°ì„ ì‹œ
            url, w = chunk, 0
        if w >= best_w:
            best_url, best_w = url, w
    return best_url.strip()

def _img_tag_best_src(img: Tag, base: str) -> str:
    """
    <img> íƒœê·¸ì—ì„œ ì“¸ë§Œí•œ srcë¥¼ ê³ ë¥¸ ë’¤ ì ˆëŒ€ URLë¡œ ë³€í™˜.
    ë ˆì´ì§€ë¡œë“œ(data-src, data-original, data-lazy-src)ì™€ srcset ëª¨ë‘ ê³ ë ¤.
    """
    if not isinstance(img, Tag):
        return ""
    cand = ""
    # 1) srcset ìµœìš°ì„ (ê°€ì¥ í° ì‚¬ì´ì¦ˆ)
    srcset = img.get("srcset") or img.get("data-srcset") or ""
    if srcset:
        cand = _parse_srcset_best(srcset)
    # 2) ë ˆì´ì§€ë¡œë“œ ì†ì„±ë“¤
    if not cand:
        for key in ("data-original", "data-src", "data-lazy-src", "data-orig-src"):
            if img.get(key):
                cand = img.get(key).strip()
                break
    # 3) ì¼ë°˜ src
    if not cand:
        cand = (img.get("src") or "").strip()
    # data:image ì œì™¸
    if cand.startswith("data:"):
        return ""
    return urljoin(base, cand) if cand else ""

def _meta_image(ss: BeautifulSoup, base: str) -> str:
    for sel in [
        "meta[property='og:image'][content]",
        "meta[name='twitter:image'][content]",
        "meta[name='twitter:image:src'][content]",
    ]:
        m = ss.select_one(sel)
        if m and m.get("content"):
            val = m["content"].strip()
            if val and not val.startswith("data:"):
                return urljoin(base, val)
    return ""

def _first_img_generic(ss: BeautifulSoup, base: str) -> str:
    # ë³¸ë¬¸ ìª½ì˜ ì²« ë²ˆì§¸ img
    for img in ss.find_all("img"):
        src = _img_tag_best_src(img, base)
        if src:
            return src
    return ""

def _domain(url: str) -> str:
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return ""

def _extract_first_image_from_article(url: str) -> str:
    """
    ë„ë©”ì¸ë³„ ê·œì¹™ â†’ ë©”íƒ€ íƒœê·¸ â†’ ì¼ë°˜ img ìˆœì„œ.
    """
    if url in FIRST_IMAGE_CACHE:
        return FIRST_IMAGE_CACHE[url]

    try:
        r = session.get(url, timeout=20)
        r.raise_for_status()
    except Exception:
        FIRST_IMAGE_CACHE[url] = ""
        return ""
    if not r.encoding or r.encoding.lower() in ("iso-8859-1", "us-ascii"):
        r.encoding = r.apparent_encoding
    ss = BeautifulSoup(r.text, "lxml")
    base = url

    host = _domain(url)

    # 1) Dongascience
    if "dongascience.com" in host:
        node = ss.select_one("div.pic_c img") or ss.select_one("div.pic img")
        if node:
            out = _img_tag_best_src(node, base)
            if out:
                FIRST_IMAGE_CACHE[url] = out
                return out

    # 2) SciTechDaily
    if "scitechdaily.com" in host:
        node = ss.select_one("figure img")
        if node:
            out = _img_tag_best_src(node, base)
            if out:
                FIRST_IMAGE_CACHE[url] = out
                return out

    # 3) TechXplore
    if "techxplore.com" in host:
        node = ss.select_one("figure.article-img img") or ss.select_one("article figure img")
        if node:
            out = _img_tag_best_src(node, base)
            if out:
                FIRST_IMAGE_CACHE[url] = out
                return out

    # 4) ë©”íƒ€ íƒœê·¸
    og = _meta_image(ss, base)
    if og:
        FIRST_IMAGE_CACHE[url] = og
        return og

    # 5) ì¼ë°˜ ì´ë¯¸ì§€
    anyimg = _first_img_generic(ss, base)
    FIRST_IMAGE_CACHE[url] = anyimg
    return anyimg

## ---------- date and source ----------

def extract_date_and_source(raw_text: str) -> tuple[str, str]:
    """
    (ë™ì•„ì‚¬ì´ì–¸ìŠ¤ / 2025.09.18.) ê°™ì€ ê¼¬ë¦¬í‘œì—ì„œ sourceì™€ date ì¶”ì¶œ
    return (date, source) / ì—†ìœ¼ë©´ ("","")
    """
    m = re.search(r"\(([^()/]+)\s*/\s*(\d{2,4})[.\-](\d{1,2})[.\-](\d{1,2})", raw_text)
    if not m:
        return "", ""
    source = m.group(1).strip()
    yyyy, mm, dd = int(m.group(2)), int(m.group(3)), int(m.group(4))
    if yyyy < 100:
        yyyy += 2000
    elif 100 <= yyyy < 1000:
        pass

    date_str = f"{yyyy:04d}-{mm:02d}-{dd:02d}"
    return date_str, source

# ë²”ìœ„ì—ì„œ (source / yyyy.mm.dd) ê¼¬ë¦¬í‘œë¥¼ ì°¾ì•„ì„œ ë°˜í™˜
DATE_SOURCE_PAT = re.compile(
    r"[ï¼ˆ(]\s*([^()ï¼ˆï¼‰/|]+?)\s*[/|]\s*(\d{2,4})[.\-](\d{1,2})[.\-](\d{1,2})\.?\s*[)ï¼‰]"
)

WEEK_LABEL_RE = re.compile(r"(\d{1,2})ì›”\s*(\d)ì£¼ì°¨")

def extract_date_source_near(root: Tag, start: Tag, end: Tag | None) -> tuple[str, str]:
    """
    start(ì œëª© p) ì´í›„ ~ end(ë‹¤ìŒ ì œëª© p) ì´ì „ ë²”ìœ„ì—ì„œ
    '(ì¶œì²˜ / YYYY.MM.DD.)' íŒ¨í„´ì„ ì°¾ì•„ (date, source) ë°˜í™˜.
    ëª» ì°¾ìœ¼ë©´ ("","")
    """
    for el in start.next_elements:
        if el is end:
            break
        if not isinstance(el, Tag):
            continue
        if is_title_paragraph(el) and el is not start:
            break

        # í…ìŠ¤íŠ¸ ë§ì€ p/spanì—ë§Œ ì‹œë„
        if el.name in {"p", "span"}:
            raw = el.get_text(" ", strip=True)
            m = DATE_SOURCE_PAT.search(raw)
            if m:
                source = m.group(1).strip()
                yyyy, mm, dd = int(m.group(2)), int(m.group(3)), int(m.group(4))
                if yyyy < 100:
                    yyyy += 2000
                date_str = f"{yyyy:04d}-{mm:02d}-{dd:02d}"  # KST ë²„í‚· YYYY-MM-DD
                return date_str, source
    return "", ""


def derive_week_key(list_date: str, period_label: str) -> str:
    """ì£¼ì°¨ í‚¤ë¥¼ YYYY-MM-N í˜•íƒœë¡œ ì •ê·œí™”."""
    year = None
    month = None
    week_no = None

    cleaned_date = list_date.replace('.', '-').strip() if list_date else ""
    if cleaned_date:
        try:
            dt = datetime.strptime(cleaned_date, "%Y-%m-%d")
            year = dt.year
            month = dt.month
            day = dt.day
            week_no = min(5, max(1, ((day - 1) // 7) + 1))
        except ValueError:
            pass

    if period_label:
        m = WEEK_LABEL_RE.search(period_label)
        if m:
            month = int(m.group(1))
            week_no = int(m.group(2))

    if year is None:
        year = datetime.now().year
    if month is None:
        month = 1
    if week_no is None:
        week_no = 1

    return f"{year:04d}-{month:02d}-{week_no}"


# ---------- main parse ----------
def parse_detail(detail_url: str, use_external_fallback: bool = False):
    try:
        r = session.get(detail_url, timeout=30)
        r.raise_for_status()
    except Exception as exc:
        print(f"[warn] failed to fetch detail: {detail_url} ({exc})", file=sys.stderr)
        return []
    if not r.encoding or r.encoding.lower() in ("iso-8859-1", "us-ascii"):
        r.encoding = r.apparent_encoding

    soup = BeautifulSoup(r.text, "lxml")
    root = soup.select_one("div.board-view-content")
    if not root: raise RuntimeError("board-view-content ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # 1) ì œëª© p ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
    title_ps: list[Tag] = [p for p in root.find_all("p", class_="MsoNormal") if is_title_paragraph(p)]

    items = []
    for i, p in enumerate(title_ps):
        link = extract_link_from_p(p)
        if not link.startswith("http"): continue

        # ì œëª© ì›ë¬¸ í…ìŠ¤íŠ¸
        title_raw = p.get_text(" ", strip=True)

        # ë‚ ì§œ + ì¶œì²˜ ê¼¬ë¦¬í‘œì—ì„œ ì¶”ì¶œ
        date, source = extract_date_and_source(title_raw)

        # ì œëª© ì •ë¦¬
        title = smart_title_clean(title_raw)
        title = strip_prefix_category(title)

        if not looks_like_title(title): continue

        # 2) ë‹¤ìŒ ì œëª© pë¥¼ ê²½ê³„ë¡œ ìš”ì•½ íƒìƒ‰
        end = title_ps[i+1] if i+1 < len(title_ps) else None
        summary = find_summary_between(root, p, end)

        # ğŸ”¹ ìš”ì•½ ë²”ìœ„ì—ì„œ ë‚ ì§œ/ì¶œì²˜ ìŠ¤ìº” (ì œëª© p ì•ˆì— ì—†ì„ ë•Œ ëŒ€ë¹„)
        date, source = extract_date_source_near(root, p, end)

        # ê·¸ë˜ë„ ëª» ì°¾ì•˜ê³ , ì œëª© rawì— ìˆëŠ” ê²½ìš°ì—” ë³´ì¡° ì¶”ì¶œ
        if not date or not source:
            title_raw = p.get_text(" ", strip=True)
            d2, s2 = extract_date_and_source(title_raw)
            if d2 and s2:
                date, source = d2, s2

        # 3) ì •ë§ ë¹„ë©´(ê·¸ë¦¬ê³  ì›í•  ë•Œë§Œ) ì™¸ë¶€ ë©”íƒ€ fallback
        if not summary and use_external_fallback:
            summary = fetch_fallback_summary_from_link(link)

        # 4) ê¸°ì‚¬ ì²« ì´ë¯¸ì§€ (NEW)
        thumbnail = _extract_first_image_from_article(link)

        items.append({
            "title": title,
            "link": link,
            "summary": summary,
            "date": date,
            "source": source,
            "thumbnail": thumbnail,  # â† ì¶”ê°€
        })

    return items

# ---------- list page (ëª©ë¡) -> detail nos ----------

LIST_URL = "https://scienceon.kisti.re.kr/trendPromo/PORTrendPromoList.do"

JS_NO_RE = re.compile(r"fn_moveTrendPromoDtl\('(\d+)'\)")

def fetch_html(url: str, params: dict | None = None) -> str:
    r = session.get(url, params=params or {}, timeout=30)
    r.raise_for_status()
    if not r.encoding or r.encoding.lower() in ("iso-8859-1", "us-ascii"):
        r.encoding = r.apparent_encoding
    return r.text

def parse_list_page(html: str) -> list[dict]:
    """
    ëª©ë¡ í˜ì´ì§€ì—ì„œ ìµœì‹  ê²Œì‹œê¸€ë“¤ì˜ trendPromoNo, ë¦¬ìŠ¤íŠ¸ íƒ€ì´í‹€, ë¦¬ìŠ¤íŠ¸ ë‚ ì§œ ì¶”ì¶œ
    return: [{"no":"260","list_title":"[9ì›” 4ì£¼ì°¨] ê¸ˆì£¼ì˜ ê³¼í•™ê¸°ìˆ ë‰´ìŠ¤(1)","list_date":"2025-09-23"}, ...]
    """
    soup = BeautifulSoup(html, "lxml")
    rows = soup.select("table.board-list-tbl tbody tr")
    out = []
    for tr in rows:
        a = tr.select_one("td.subject a[href]")
        if not a:
            continue
        m = JS_NO_RE.search(a.get("href", ""))
        if not m:
            continue
        no = m.group(1)
        list_title = clean_text(a.get_text(" ", strip=True))
        list_date = clean_text((tr.select_one("td.date") or {}).get_text(" ", strip=True) if tr.select_one("td.date") else "")

        pm = re.search(r"\[([0-9]+ì›”\s*[0-9]+ì£¼ì°¨)\]", list_title)
        period_label = pm.group(1) if pm else list_title

        out.append({
            "no": no, 
            "period_label": period_label, 
            "list_date": list_date
        })
    return out

def crawl_from_list(pages: int = 1, limit: int | None = None, use_external_fallback: bool = False) -> list[dict]:
    """
    ëª©ë¡ í˜ì´ì§€ë¥¼ ì•ì—ì„œë¶€í„° `pages` ì¥ í›‘ì–´ ìµœì‹  ê¸€ë“¤ì˜ ìƒì„¸ë¥¼ í¬ë¡¤ë§.
    - pages: 1ì´ë©´ ì²« í˜ì´ì§€(ìµœì‹  10ê°œ)ë§Œ
    - limit: ì´ ìˆ˜ì§‘ ìƒí•œ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
    """
    # KISTIëŠ” ë³´í†µ pageIndex íŒŒë¼ë¯¸í„°ë¡œ í˜ì´ì§• (ì—†ìœ¼ë©´ 1í˜ì´ì§€)
    all_list_items: list[dict] = []
    for page in range(1, pages + 1):
        html = fetch_html(LIST_URL, params={"pageIndex": page})
        items = parse_list_page(html)
        if not items:
            break
        all_list_items.extend(items)
        if limit and len(all_list_items) >= limit:
            all_list_items = all_list_items[:limit]
            break

    results: list[dict] = []
    seen_links: set[str] = set()
    for li in all_list_items:
        detail_url = BASE_DETAIL.format(no=li["no"])
        detail_items = parse_detail(detail_url, use_external_fallback=True)
        # í•´ë‹¹ ìƒì„¸ ê¸€ ì•ˆì— ì—¬ëŸ¬ ê¸°ì‚¬(title/link/summaryâ€¦)ê°€ ë“¤ì–´ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë³‘í•©
        for d in detail_items:
            period_label = li["period_label"]
            list_date = li.get("list_date", "")

            if not d.get("date") and list_date:
                if re.match(r"^\d{4}[.\-]\d{2}[.\-]\d{2}$", list_date):
                    d["date"] = list_date.replace(".", "-")

            week_key = derive_week_key(list_date, period_label)
            summary_text = truncate_summary(d.get("summary", ""))
            if not summary_text:
                summary_text = "ìš”ì•½ ì—†ìŒ"

            if d["link"] in seen_links:
                continue
            seen_links.add(d["link"])

            source_name = d.get("source") or DEFAULT_SOURCE

            category = categorize(d.get("title"), CATEGORY_DEFAULT)

            results.append({
                "id": sha1_hex(f"{DEFAULT_SOURCE}:{week_key}:{d['title']}:{d['link']}"),
                "title": d["title"],
                "link": d["link"],
                "summary": summary_text,
                "week": week_key,
                "date": d.get("date") or (list_date.replace(".", "-") if list_date else ""),
                "source": source_name,
                "category": category,
                "period_label": period_label,
                "thumbnail": d.get("thumbnail") or None,
            })
    return results


# ---------- CLI ----------

def print_usage():
    print("Usage:")
    print("  python science_on_scraper.py list               # ìµœì‹  10ê°œ(1í˜ì´ì§€) ìƒì„¸ í¬ë¡¤ë§")
    print("  python science_on_scraper.py list 2             # 2í˜ì´ì§€(ìµœëŒ€ 20ê°œ) í¬ë¡¤ë§")
    print("  python science_on_scraper.py list 1 30          # 1í˜ì´ì§€ì—ì„œ ìµœëŒ€ 30ê°œ(ë„˜ì¹˜ë©´ ë‹¤ìŒ í˜ì´ì§€ë¡œ) í¬ë¡¤ë§")
    print("  python science_on_scraper.py 260                # ë‹¨ì¼ ìƒì„¸ í˜ì´ì§€ë§Œ (ê¸°ì¡´ ë™ì‘)")

def main():
    # ê¸°ì¡´ ë‹¨ì¼ ìƒì„¸ ë™ì‘ ìœ ì§€ + ëª©ë¡ ëª¨ë“œ ì¶”ê°€
    if len(sys.argv) >= 2 and sys.argv[1].lower() == "list":
        pages = int(sys.argv[2]) if len(sys.argv) >= 3 else 1
        limit = int(sys.argv[3]) if len(sys.argv) >= 4 else None
        data = crawl_from_list(pages=pages, limit=limit, use_external_fallback=False)
        print(json.dumps(data, ensure_ascii=False, indent=2))
        return

    # ë‹¨ì¼ ìƒì„¸ (ê¸°ì¡´)
    no = sys.argv[1] if len(sys.argv) >= 2 else "260"
    url = BASE_DETAIL.format(no=no)
    data = parse_detail(url, use_external_fallback=False)
    print(json.dumps(data, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()
