# science_on_scraper.py
"""scienceON ì£¼ê°„ íŠ¸ë Œë“œ í˜ì´ì§€ë¥¼ ìˆ˜ì§‘í•´ êµ¬ì¡°í™”ëœ ê¸°ì‚¬ JSONì„ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸."""

import hashlib
import re
import sys
import json
from datetime import datetime
from urllib.parse import urlparse, urljoin

import requests
from requests.adapters import HTTPAdapter, Retry
from bs4 import BeautifulSoup, Tag

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
DEFAULT_SOURCE = "scienceON"

# --- ì¹´í…Œê³ ë¦¬ ê·œì¹™ (source ê¸°ë³¸ê°’ + ì œëª© í‚¤ì›Œë“œ ë®ì–´ì“°ê¸°) ---
CATEGORY_DEFAULT = "ê³¼í•™ê¸°ìˆ "
# `categorize`: ê¸°ì‚¬ ì œëª©ì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ ê³¨ë¼ì£¼ëŠ” ê°„ë‹¨í•œ ê·œì¹™ í•¨ìˆ˜ì…ë‹ˆë‹¤.
CATEGORY_RULES = [
    (re.compile(r"(AI|ì¸ê³µì§€ëŠ¥|GPT|LLM|ë”¥ëŸ¬ë‹)", re.I), "AI"),
    (re.compile(r"(ë³´ì•ˆ|í•´í‚¹|ìœ ì¶œ|ì·¨ì•½|ëœì„¬)", re.I), "ë³´ì•ˆ"),
    (re.compile(r"(ë°˜ë„ì²´|ì¹©|íŒŒìš´ë“œë¦¬|í€€í…€|ì—”ë¹„ë””ì•„|ì–‘ì)", re.I), "ë°˜ë„ì²´"),
    (re.compile(r"(ë¡œë´‡|ë¡œë³´í‹±ìŠ¤)", re.I), "ë¡œë´‡"),
    (re.compile(r"(ìƒëª…|ì œì•½|ë°±ì‹ |ìœ ì „ì²´|ë¯¸ìƒë¬¼|ë°”ì´ì˜¤|ì‹ ì•½|í•­ì•”ì œ|ì•ˆì•½|ì¤„ê¸°ì„¸í¬|ì¥|ì²™ìˆ˜|ë¯¸íŠ¸ì½˜ë“œë¦¬ì•„|ì•”|ê°„|ê·¼ìœ¡|ê³ í˜ˆì••|ìˆ˜ë©´|DNA|ì„¸í¬|ë¨¸ë¦¬ì¹´ë½|ì—¬ë“œë¦„|ì•Œì¸ í•˜ì´ë¨¸|í•­ìƒì œ|ë°”ì´ëŸ¬ìŠ¤|ë°•í…Œë¦¬ì•„|ê³¨|ì•Œí…Œì˜¤ì  |ì…€íŠ¸ë¦¬ì˜¨|íŒ¬ì  |ì„ìƒ|ì˜ì•½|ì²˜ë°©)", re.I), "ìƒëª…ê³¼í•™"),
    (re.compile(r"(ë°°í„°ë¦¬|ì „ì§€|íƒ‘ë¨¸í‹°ë¦¬ì–¼)", re.I), "ë°°í„°ë¦¬"),
]

# categorize: ì œëª© ë¬¸ìì—´ì„ ë°›ì•„ scienceONì—ì„œ ì‚¬ìš©í•  ì¹´í…Œê³ ë¦¬ë¥¼ ëŒë ¤ì¤ë‹ˆë‹¤.
def categorize(title: str, fallback: str | None = None) -> str:
    """Return a category label for a scienceON headline."""
    base = fallback or CATEGORY_DEFAULT
    if not title:
        return base
    for pattern, label in CATEGORY_RULES:
        if pattern.search(title):
            return label
    return base

session = requests.Session()
retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])
session.mount("https://", HTTPAdapter(max_retries=retries))
session.mount("http://", HTTPAdapter(max_retries=retries))
session.headers.update(HEADERS)
BASE_DETAIL = "https://scienceon.kisti.re.kr/trendPromo/PORTrendPromoDtl.do?trendPromoNo={no}"

# ---------- text utils ----------
# clean_text: ë“¤ì‘¥ë‚ ì‘¥í•œ ê³µë°±ì„ ì •ë¦¬í•´ ê¹”ë”í•œ ë¬¸ìì—´ë¡œ ë§Œë“­ë‹ˆë‹¤.
def clean_text(s: str) -> str:
    """ë¬¸ìì—´ ë‚´ë¶€ ê³µë°±ì„ ì •ë¦¬í•´ ê¹”ë”í•œ í…ìŠ¤íŠ¸ë¥¼ ë§Œë“ ë‹¤."""
    if not s: return ""
    s = s.replace("\xa0", " ").replace("\u00a0", " ")
    s = re.sub(r"\s+", " ", s)
    return s.strip()


# sha1_hex: ë¬¸ìì—´ì„ SHA1 í•´ì‹œë¡œ ë³€í™˜í•´ ì•ˆì •ì ì¸ IDë¥¼ ë§Œë“­ë‹ˆë‹¤.
def sha1_hex(value: str) -> str:
    """ì•ˆì •ì ì¸ ID ìƒì„±ì„ ìœ„í•´ SHA1 í•´ì‹œ ê°’ì„ ê³„ì‚°í•œë‹¤."""
    return hashlib.sha1(value.encode("utf-8")).hexdigest()


SENTENCE_ENDINGS = ['ë‹¤.', 'ë‹¤?', 'ë‹¤!', '.', '!', '?', 'â€¦']


# ensure_sentence_boundary: ìš”ì•½ì´ ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ëŠê¸°ì§€ ì•Šë„ë¡ ë§ˆë¬´ë¦¬ë¥¼ ë‹¤ë“¬ìŠµë‹ˆë‹¤.
def ensure_sentence_boundary(text: str, truncated: bool = False) -> str:
    """ë¬¸ì¥ ê²½ê³„ì— ë§ì¶° ìš”ì•½ì„ ìë¥´ê³  í•„ìš”í•œ ê²½ìš° ë§ì¤„ì„í‘œë¥¼ ë¶™ì¸ë‹¤."""
    text = (text or '').strip()
    if not text:
        return text
    end_positions: list[int] = []
    for ending in SENTENCE_ENDINGS:
        idx = text.rfind(ending)
        if idx != -1:
            end_positions.append(idx + len(ending))
    if end_positions:
        return text[:max(end_positions)].strip()
    if truncated:
        return text.rstrip('.') + 'â€¦'
    return text


# truncate_summary: ë¬¸ì¥ ìˆ˜Â·ê¸€ì ìˆ˜ ì œí•œì„ ì ìš©í•´ ì§§ì€ ìš”ì•½ë¬¸ì„ ë§Œë“­ë‹ˆë‹¤.
def truncate_summary(text: str, max_sentences: int = 3, max_chars: int = 180) -> str:
    """ë¬¸ì¥ ìˆ˜ì™€ ê¸€ì ìˆ˜ ì œí•œì„ ì ìš©í•´ ê°„ê²°í•œ ìš”ì•½ì„ ë§Œë“ ë‹¤."""
    if not text:
        return ""
    normalized = text.replace("\n", " ")
    pieces = re.split(r"(?<=[.!?\u3002\uFF01\uFF1F])\s+", normalized)
    sentences: list[str] = []
    for piece in pieces:
        part = clean_text(piece)
        if part:
            sentences.append(part)
        if len(sentences) >= max_sentences:
            break
    if not sentences:
        sentences = [clean_text(text)]

    selected: list[str] = []
    truncated = False
    for sent in sentences:
        candidate = " ".join(selected + [sent]) if selected else sent
        if len(candidate) > max_chars:
            truncated = True
            break
        selected.append(sent)

    if not selected:
        truncated = True
        summary = sentences[0][:max_chars].strip()
    else:
        summary = " ".join(selected).strip()
        if len(summary) > max_chars:
            truncated = True
            summary = summary[:max_chars].strip()

    summary = ensure_sentence_boundary(summary, truncated)
    if not summary:
        fallback = ensure_sentence_boundary(sentences[0][:max_chars].strip(), True)
        return fallback or ""
    return summary

# looks_like_title: ë¬¸ìì—´ì´ ê¸°ì‚¬ ì œëª©ìœ¼ë¡œ ë³´ê¸° ì ë‹¹í•œì§€ í™•ì¸í•©ë‹ˆë‹¤.
def looks_like_title(t: str) -> bool:
    if not t or len(t) < 4: return False
    if t.lower() in {"new", "(new)"}: return False
    return True

# smart_title_clean: ì œëª©ì—ì„œ ë¶ˆí•„ìš”í•œ ê¼¬ë¦¬í‘œë‚˜ ê¸´ ë¶€ë¶„ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
def smart_title_clean(s: str) -> str:
    # ë¶ˆë¦¿ ì´í›„/ë‚ ì§œ ê¼¬ë¦¬í‘œ ì œê±°
    s = clean_text(s)
    if "ã†" in s: s = s.split("ã†", 1)[0].strip()
    s = re.sub(r"\(\s*[^()]*\d{2,4}[.\-/]\d{1,2}([.\-/]\d{1,2})?[^()]*\)\s*$", "", s).strip()
    if len(s) > 180: s = s[:180].rstrip()
    return s

# strip_anchor_text: íƒœê·¸ ë‚´ë¶€ì˜ ë§í¬ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ë½‘ì•„ëƒ…ë‹ˆë‹¤.
def strip_anchor_text(node: Tag) -> str:
    soup = BeautifulSoup(str(node), "lxml")
    for a in soup.find_all("a"): a.decompose()
    return soup.get_text(" ", strip=True)

# has_korean: ë¬¸ìì—´ì— í•œê¸€ì´ í¬í•¨ëëŠ”ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
def has_korean(t: str) -> bool:
    return bool(re.search(r"[ê°€-í£]", t or ""))

# ---------- page structure helpers ----------
# is_title_paragraph: í˜„ì¬ ë‹¨ë½ì´ ê¸°ì‚¬ ì œëª© ë‹¨ë½ í˜•íƒœì¸ì§€ ê²€ì‚¬í•©ë‹ˆë‹¤.
def is_title_paragraph(p: Tag) -> bool:
    return (p.name == "p" and "MsoNormal" in (p.get("class") or []) and p.find("a", href=True) is not None)

# extract_link_from_p: ì œëª© ë‹¨ë½ì—ì„œ ì‹¤ì œ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
def extract_link_from_p(p: Tag) -> str:
    for a in p.find_all("a", href=True):
        href = (a.get("href") or "").strip()
        if href.startswith("http"): return href
    return ""

# extract_title_from_p: ì œëª© ë‹¨ë½ì—ì„œ ë³´ê¸° ì¢‹ì€ ì œëª© ë¬¸êµ¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
def extract_title_from_p(p: Tag) -> str:
    # aê°€ ë“¤ì–´ìˆëŠ” ê°€ì¥ ê°€ê¹Œìš´ ì»¨í…Œì´ë„ˆ(span/strong/em) ìš°ì„ 
    a = p.find("a", href=True)
    if a:
        cur, depth = a.parent, 0
        while isinstance(cur, Tag) and depth < 5:
            if cur.name in {"span", "strong", "em"}:
                t = smart_title_clean(strip_anchor_text(cur))
                if looks_like_title(t): return t
            if cur == p: break
            cur = cur.parent; depth += 1
    # pì—ì„œ a ì œê±°
    t2 = smart_title_clean(strip_anchor_text(p))
    if looks_like_title(t2): return t2
    # ìµœí›„: p ì „ì²´
    title = smart_title_clean(p.get_text(" ", strip=True))
    # ì¹´í…Œê³ ë¦¬ í”„ë¦¬í”½ìŠ¤ ì œê±°
    title = strip_prefix_category(title)
    return title

# strip_prefix_category: ì œëª© ì•ì— ë¶™ì€ ì¹´í…Œê³ ë¦¬ í‘œê¸°ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
def strip_prefix_category(title: str) -> str:
    """
    ì œëª© ë§¨ ì•ì˜ '- ( ... )' ë˜ëŠ” '( ... )' íŒ¨í„´ì„ ì œê±°.
    ì˜ˆ: '- ( ğŸ’¥ ì–‘ì ) ê±°ì‹œì„¸ê³„...' â†’ 'ê±°ì‹œì„¸ê³„...'
    """
    # ë§¨ ì•ì˜ ëŒ€ì‹œì™€ ê´„í˜¸ ë¸”ë¡ ì œê±°
    title = re.sub(r"^\s*-\s*\([^)]*\)\s*", "", title)
    title = re.sub(r"^\s*\([^)]*\)\s*", "", title)
    return title.strip()


# ---------- summary finder ----------
BULLET_START = re.compile(r"^[\s\u00A0]*[ã†Â·â€¢\-]+[\s\u00A0]*")

# split_first_bullet_line: ë¶ˆë¦¿ ë¬¸ë‹¨ì—ì„œ ì²« ë²ˆì§¸ í•­ëª©ë§Œ ì˜ë¼ëƒ…ë‹ˆë‹¤.
def split_first_bullet_line(text: str) -> str:
    """
    ê¸´ ë¬¸ë‹¨ì— ë¶ˆë¦¿ì´ ì—¬ëŸ¬ ê°œ ì´ì–´ì§ˆ ë•Œ, 'ì²« ë¶ˆë¦¿'ë§Œ í•œ ì¤„ ìš”ì•½ìœ¼ë¡œ.
    ì˜ˆ: 'ã† ... - ã† ...' í˜•íƒœ â†’ ì²« ë¶ˆë¦¿ ì¡°ê°ë§Œ.
    """
    # ë¨¼ì € ë§¨ ì• ë¶ˆë¦¿ ì œê±°
    t = BULLET_START.sub("", text).strip()
    # ë‹¤ìŒ ë¶ˆë¦¿/ëŒ€ì‹œ ì•ê¹Œì§€ ìë¥´ê¸°
    t = re.split(r"\s(?:[ã†Â·â€¢\-]\s)", t, maxsplit=1)[0]
    return clean_text(t)

# find_summary_between: ì œëª©ê³¼ ë‹¤ìŒ ì œëª© ì‚¬ì´ì—ì„œ ìš”ì•½ ë¬¸ì¥ì„ ì°¾ì•„ì¤ë‹ˆë‹¤.
def find_summary_between(root: Tag, start: Tag, end: Tag | None) -> str:
    """
    start(ì œëª© p) ì´í›„ë¶€í„° end(ë‹¤ìŒ ì œëª© p) ì´ì „ê¹Œì§€ ë²”ìœ„ë¥¼ í›‘ì–´ ìš”ì•½ í›„ë³´ë¥¼ ì°¾ëŠ”ë‹¤.
    ìš°ì„ ìˆœìœ„:
      1) ë¶ˆë¦¿ í¬í•¨ p (í•œê¸€ ìš°ì„ )
      2) ë§í¬ ì—†ëŠ” 14px ìŠ¤íƒ€ì¼ p (í•œê¸€ ìš°ì„ )
      3) ë§í¬ ì—†ëŠ” ì¼ë°˜ p (í•œê¸€ ìš°ì„ , 10~300ì)
    ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
    """
    # í›„ë³´ ë²„í‚·
    bullet_ko = None; bullet_any = None
    body14_ko = None; body14_any = None
    body_ko = None; body_any = None

    # next_elementsë¥¼ ì¨ì„œ h2 ê²½ê³„ë„ í†µê³¼
    for el in start.next_elements:
        if el is end: break
        if not isinstance(el, Tag): continue

        # ë‹¤ìŒ ì œëª© pë¥¼ ë§Œë‚˜ë©´ ì¢…ë£Œ
        if is_title_paragraph(el) and el is not start:
            break

        if el.name == "p" and "MsoNormal" in (el.get("class") or []):
            has_link = el.find("a", href=True) is not None
            raw = el.get_text(" ", strip=True)
            txt = clean_text(raw)

            # 1) ë¶ˆë¦¿ ì¼€ì´ìŠ¤
            if BULLET_START.search(txt):
                one = split_first_bullet_line(txt)
                if has_korean(one) and not bullet_ko: bullet_ko = one
                if not bullet_any: bullet_any = one
                continue

            # 2) 14px ë³¸ë¬¸ & ë§í¬ ì—†ìŒ
            if not has_link and "font-size: 14px" in (el.get("style") or ""):
                if has_korean(txt) and not body14_ko: body14_ko = txt
                if not body14_any: body14_any = txt
                continue

            # 3) ì¼ë°˜ ë³¸ë¬¸ & ë§í¬ ì—†ìŒ (ë„ˆë¬´ ê¸´ ê±´ ì œì™¸)
            if not has_link and 10 <= len(txt) <= 300:
                if has_korean(txt) and not body_ko: body_ko = txt
                if not body_any: body_any = txt

    return bullet_ko or body14_ko or body_ko or bullet_any or body14_any or body_any or ""

# ---------- optional external fallback ----------
# fetch_fallback_summary_from_link: ê¸°ì‚¬ ì›ë¬¸ì—ì„œ ë©”íƒ€ ì„¤ëª…ì„ ê°€ì ¸ì˜¤ëŠ” ë³´ì¡° ìš”ì•½ì…ë‹ˆë‹¤.
def fetch_fallback_summary_from_link(link: str) -> str:
    try:
        rr = session.get(link, timeout=30)
        rr.raise_for_status()
    except Exception:
        return ""
    if not rr.encoding or rr.encoding.lower() in ("iso-8859-1", "us-ascii"):
        rr.encoding = rr.apparent_encoding
    ss = BeautifulSoup(rr.text, "lxml")
    for sel in ["meta[property='og:description'][content]", "meta[name='description'][content]"]:
        m = ss.select_one(sel)
        if m and m.get("content"):
            return clean_text(m["content"])
    return ""

# ---------- image helpers (NEW) ----------

FIRST_IMAGE_CACHE: dict[str, str] = {}

# _parse_srcset_best: srcset ì†ì„±ì—ì„œ ê°€ì¥ í° ì´ë¯¸ì§€ë¥¼ ê³ ë¦…ë‹ˆë‹¤.
def _parse_srcset_best(srcset: str) -> str:
    """
    srcsetì—ì„œ ê°€ì¥ í° wë¥¼ ê°€ì§„ ì´ë¯¸ì§€ URL ë°˜í™˜. ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´.
    """
    best_url, best_w = "", -1
    for part in srcset.split(","):
        chunk = part.strip()
        if not chunk:
            continue
        # "url 777w" í˜•íƒœ
        m = re.match(r"(.+?)\s+(\d+)w", chunk)
        if m:
            url = m.group(1).strip()
            try:
                w = int(m.group(2))
            except ValueError:
                w = -1
        else:
            # w í‘œê¸°ê°€ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ê²ƒì„ ìš°ì„ ì‹œ
            url, w = chunk, 0
        if w >= best_w:
            best_url, best_w = url, w
    return best_url.strip()

# _img_tag_best_src: <img> íƒœê·¸ ì† ì—¬ëŸ¬ í›„ë³´ ì¤‘ ì‚¬ìš©í•  ì¸ë„¤ì¼ URLì„ ì„ íƒí•©ë‹ˆë‹¤.
def _img_tag_best_src(img: Tag, base: str) -> str:
    """
    <img> íƒœê·¸ì—ì„œ ì“¸ë§Œí•œ srcë¥¼ ê³ ë¥¸ ë’¤ ì ˆëŒ€ URLë¡œ ë³€í™˜.
    ë ˆì´ì§€ë¡œë“œ(data-src, data-original, data-lazy-src)ì™€ srcset ëª¨ë‘ ê³ ë ¤.
    """
    if not isinstance(img, Tag):
        return ""
    cand = ""
    # 1) srcset ìµœìš°ì„ (ê°€ì¥ í° ì‚¬ì´ì¦ˆ)
    srcset = img.get("srcset") or img.get("data-srcset") or ""
    if srcset:
        cand = _parse_srcset_best(srcset)
    # 2) ë ˆì´ì§€ë¡œë“œ ì†ì„±ë“¤
    if not cand:
        for key in ("data-original", "data-src", "data-lazy-src", "data-orig-src"):
            if img.get(key):
                cand = img.get(key).strip()
                break
    # 3) ì¼ë°˜ src
    if not cand:
        cand = (img.get("src") or "").strip()
    # data:image ì œì™¸
    if cand.startswith("data:"):
        return ""
    return urljoin(base, cand) if cand else ""

# _meta_image: ë©”íƒ€ íƒœê·¸ ì† ëŒ€í‘œ ì´ë¯¸ì§€ ì£¼ì†Œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
def _meta_image(ss: BeautifulSoup, base: str) -> str:
    for sel in [
        "meta[property='og:image'][content]",
        "meta[name='twitter:image'][content]",
        "meta[name='twitter:image:src'][content]",
    ]:
        m = ss.select_one(sel)
        if m and m.get("content"):
            val = m["content"].strip()
            if val and not val.startswith("data:"):
                return urljoin(base, val)
    return ""

# _first_img_generic: ë³¸ë¬¸ì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì•„ ì¸ë„¤ì¼ë¡œ ì”ë‹ˆë‹¤.
def _first_img_generic(ss: BeautifulSoup, base: str) -> str:
    # ë³¸ë¬¸ ìª½ì˜ ì²« ë²ˆì§¸ img
    for img in ss.find_all("img"):
        src = _img_tag_best_src(img, base)
        if src:
            return src
    return ""

# _domain: URLì—ì„œ ë„ë©”ì¸ ë¶€ë¶„ë§Œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
def _domain(url: str) -> str:
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return ""

# _extract_first_image_from_article: ê¸°ì‚¬ í˜ì´ì§€ë¥¼ ì—´ì–´ ì¸ë„¤ì¼ í›„ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
def _extract_first_image_from_article(url: str) -> str:
    """
    ë„ë©”ì¸ë³„ ê·œì¹™ â†’ ë©”íƒ€ íƒœê·¸ â†’ ì¼ë°˜ img ìˆœì„œ.
    """
    if url in FIRST_IMAGE_CACHE:
        return FIRST_IMAGE_CACHE[url]

    try:
        r = session.get(url, timeout=30)
        r.raise_for_status()
    except Exception:
        FIRST_IMAGE_CACHE[url] = ""
        return ""
    if not r.encoding or r.encoding.lower() in ("iso-8859-1", "us-ascii"):
        r.encoding = r.apparent_encoding
    ss = BeautifulSoup(r.text, "lxml")
    base = url

    host = _domain(url)

    # 1) Dongascience
    if "dongascience.com" in host:
        node = ss.select_one("div.pic_c img") or ss.select_one("div.pic img")
        if node:
            out = _img_tag_best_src(node, base)
            if out:
                FIRST_IMAGE_CACHE[url] = out
                return out

    # 2) SciTechDaily
    if "scitechdaily.com" in host:
        node = ss.select_one("figure img")
        if node:
            out = _img_tag_best_src(node, base)
            if out:
                FIRST_IMAGE_CACHE[url] = out
                return out

    # 3) TechXplore
    if "techxplore.com" in host:
        node = ss.select_one("figure.article-img img") or ss.select_one("article figure img")
        if node:
            out = _img_tag_best_src(node, base)
            if out:
                FIRST_IMAGE_CACHE[url] = out
                return out

    # 4) ë©”íƒ€ íƒœê·¸
    og = _meta_image(ss, base)
    if og:
        FIRST_IMAGE_CACHE[url] = og
        return og

    # 5) ì¼ë°˜ ì´ë¯¸ì§€
    anyimg = _first_img_generic(ss, base)
    FIRST_IMAGE_CACHE[url] = anyimg
    return anyimg

## ---------- date and source ----------

# extract_date_and_source: '(ì¶œì²˜ / ë‚ ì§œ)' ê¼¬ë¦¬í‘œì—ì„œ ë‚ ì§œì™€ ì¶œì²˜ë¥¼ ë½‘ìŠµë‹ˆë‹¤.
def extract_date_and_source(raw_text: str) -> tuple[str, str]:
    """
    (ë™ì•„ì‚¬ì´ì–¸ìŠ¤ / 2025.09.18.) ê°™ì€ ê¼¬ë¦¬í‘œì—ì„œ sourceì™€ date ì¶”ì¶œ
    return (date, source) / ì—†ìœ¼ë©´ ("","")
    """
    m = re.search(r"\(([^()/]+)\s*/\s*(\d{2,4})[.\-](\d{1,2})[.\-](\d{1,2})", raw_text)
    if not m:
        return "", ""
    source = m.group(1).strip()
    yyyy, mm, dd = int(m.group(2)), int(m.group(3)), int(m.group(4))
    if yyyy < 100:
        yyyy += 2000
    elif 100 <= yyyy < 1000:
        pass

    date_str = f"{yyyy:04d}-{mm:02d}-{dd:02d}"
    return date_str, source

# ë²”ìœ„ì—ì„œ (source / yyyy.mm.dd) ê¼¬ë¦¬í‘œë¥¼ ì°¾ì•„ì„œ ë°˜í™˜
DATE_SOURCE_PAT = re.compile(
    r"[ï¼ˆ(]\s*([^()ï¼ˆï¼‰/|]+?)\s*[/|]\s*(\d{2,4})[.\-](\d{1,2})[.\-](\d{1,2})\.?\s*[)ï¼‰]"
)

WEEK_LABEL_RE = re.compile(r"(\d{1,2})ì›”\s*(\d)ì£¼ì°¨")

# extract_date_source_near: ì œëª© ê·¼ì²˜ì—ì„œ ë‚ ì§œÂ·ì¶œì²˜ ì •ë³´ë¥¼ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í•©ë‹ˆë‹¤.
def extract_date_source_near(root: Tag, start: Tag, end: Tag | None) -> tuple[str, str]:
    """
    start(ì œëª© p) ì´í›„ ~ end(ë‹¤ìŒ ì œëª© p) ì´ì „ ë²”ìœ„ì—ì„œ
    '(ì¶œì²˜ / YYYY.MM.DD.)' íŒ¨í„´ì„ ì°¾ì•„ (date, source) ë°˜í™˜.
    ëª» ì°¾ìœ¼ë©´ ("","")
    """
    for el in start.next_elements:
        if el is end:
            break
        if not isinstance(el, Tag):
            continue
        if is_title_paragraph(el) and el is not start:
            break

        # í…ìŠ¤íŠ¸ ë§ì€ p/spanì—ë§Œ ì‹œë„
        if el.name in {"p", "span"}:
            raw = el.get_text(" ", strip=True)
            m = DATE_SOURCE_PAT.search(raw)
            if m:
                source = m.group(1).strip()
                yyyy, mm, dd = int(m.group(2)), int(m.group(3)), int(m.group(4))
                if yyyy < 100:
                    yyyy += 2000
                date_str = f"{yyyy:04d}-{mm:02d}-{dd:02d}"  # KST ë²„í‚· YYYY-MM-DD
                return date_str, source
    return "", ""


# derive_week_key: ê¸°ì‚¬ ëª©ë¡ ë‚ ì§œì™€ ì£¼ì°¨ ë¼ë²¨ë¡œ ì£¼ì°¨ í‚¤ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
def derive_week_key(list_date: str, period_label: str) -> str:
    """ì£¼ì°¨ í‚¤ë¥¼ YYYY-MM-N í˜•íƒœë¡œ ì •ê·œí™”."""
    year = None
    month = None
    week_no = None

    cleaned_date = list_date.replace('.', '-').strip() if list_date else ""
    if cleaned_date:
        try:
            dt = datetime.strptime(cleaned_date, "%Y-%m-%d")
            year = dt.year
            month = dt.month
            day = dt.day
            week_no = min(5, max(1, ((day - 1) // 7) + 1))
        except ValueError:
            pass

    if period_label:
        m = WEEK_LABEL_RE.search(period_label)
        if m:
            month = int(m.group(1))
            week_no = int(m.group(2))

    if year is None:
        year = datetime.now().year
    if month is None:
        month = 1
    if week_no is None:
        week_no = 1

    return f"{year:04d}-{month:02d}-{week_no}"


# ---------- main parse ----------
# parse_detail: scienceON ìƒì„¸ í˜ì´ì§€ì—ì„œ ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
def parse_detail(detail_url: str, use_external_fallback: bool = False):
    try:
        r = session.get(detail_url, timeout=30)
        r.raise_for_status()
    except Exception as exc:
        print(f"[warn] failed to fetch detail: {detail_url} ({exc})", file=sys.stderr)
        return []
    if not r.encoding or r.encoding.lower() in ("iso-8859-1", "us-ascii"):
        r.encoding = r.apparent_encoding

    soup = BeautifulSoup(r.text, "lxml")
    root = soup.select_one("div.board-view-content")
    if not root: raise RuntimeError("board-view-content ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # 1) ì œëª© p ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
    title_ps: list[Tag] = [p for p in root.find_all("p", class_="MsoNormal") if is_title_paragraph(p)]

    items = []
    for i, p in enumerate(title_ps):
        link = extract_link_from_p(p)
        if not link.startswith("http"): continue

        # ì œëª© ì›ë¬¸ í…ìŠ¤íŠ¸
        title_raw = p.get_text(" ", strip=True)

        # ë‚ ì§œ + ì¶œì²˜ ê¼¬ë¦¬í‘œì—ì„œ ì¶”ì¶œ
        date, source = extract_date_and_source(title_raw)

        # ì œëª© ì •ë¦¬
        title = smart_title_clean(title_raw)
        title = strip_prefix_category(title)

        if not looks_like_title(title): continue

        # 2) ë‹¤ìŒ ì œëª© pë¥¼ ê²½ê³„ë¡œ ìš”ì•½ íƒìƒ‰
        end = title_ps[i+1] if i+1 < len(title_ps) else None
        summary = find_summary_between(root, p, end)

        # ğŸ”¹ ìš”ì•½ ë²”ìœ„ì—ì„œ ë‚ ì§œ/ì¶œì²˜ ìŠ¤ìº” (ì œëª© p ì•ˆì— ì—†ì„ ë•Œ ëŒ€ë¹„)
        date, source = extract_date_source_near(root, p, end)

        # ê·¸ë˜ë„ ëª» ì°¾ì•˜ê³ , ì œëª© rawì— ìˆëŠ” ê²½ìš°ì—” ë³´ì¡° ì¶”ì¶œ
        if not date or not source:
            title_raw = p.get_text(" ", strip=True)
            d2, s2 = extract_date_and_source(title_raw)
            if d2 and s2:
                date, source = d2, s2

        # 3) ì •ë§ ë¹„ë©´(ê·¸ë¦¬ê³  ì›í•  ë•Œë§Œ) ì™¸ë¶€ ë©”íƒ€ fallback
        if not summary and use_external_fallback:
            summary = fetch_fallback_summary_from_link(link)

        # 4) ê¸°ì‚¬ ì²« ì´ë¯¸ì§€ (NEW)
        thumbnail = _extract_first_image_from_article(link)

        items.append({
            "title": title,
            "link": link,
            "summary": summary,
            "date": date,
            "source": source,
            "thumbnail": thumbnail,  # â† ì¶”ê°€
        })

    return items

# ---------- list page (ëª©ë¡) -> detail nos ----------

LIST_URL = "https://scienceon.kisti.re.kr/trendPromo/PORTrendPromoList.do"

JS_NO_RE = re.compile(r"fn_moveTrendPromoDtl\('(\d+)'\)")

# fetch_html: ì§€ì •í•œ URLì„ í˜¸ì¶œí•´ HTML ë¬¸ìì—´ì„ ëŒë ¤ì¤ë‹ˆë‹¤.
def fetch_html(url: str, params: dict | None = None) -> str:
    r = session.get(url, params=params or {}, timeout=30)
    r.raise_for_status()
    if not r.encoding or r.encoding.lower() in ("iso-8859-1", "us-ascii"):
        r.encoding = r.apparent_encoding
    return r.text

# parse_list_page: ëª©ë¡ í˜ì´ì§€ HTMLì—ì„œ ìƒì„¸ í˜ì´ì§€ ë²ˆí˜¸ì™€ ì œëª©ì„ ëª¨ìë‹ˆë‹¤.
def parse_list_page(html: str) -> list[dict]:
    """
    ëª©ë¡ í˜ì´ì§€ì—ì„œ ìµœì‹  ê²Œì‹œê¸€ë“¤ì˜ trendPromoNo, ë¦¬ìŠ¤íŠ¸ íƒ€ì´í‹€, ë¦¬ìŠ¤íŠ¸ ë‚ ì§œ ì¶”ì¶œ
    return: [{"no":"260","list_title":"[9ì›” 4ì£¼ì°¨] ê¸ˆì£¼ì˜ ê³¼í•™ê¸°ìˆ ë‰´ìŠ¤(1)","list_date":"2025-09-23"}, ...]
    """
    soup = BeautifulSoup(html, "lxml")
    rows = soup.select("table.board-list-tbl tbody tr")
    out = []
    for tr in rows:
        a = tr.select_one("td.subject a[href]")
        if not a:
            continue
        m = JS_NO_RE.search(a.get("href", ""))
        if not m:
            continue
        no = m.group(1)
        list_title = clean_text(a.get_text(" ", strip=True))
        list_date = clean_text((tr.select_one("td.date") or {}).get_text(" ", strip=True) if tr.select_one("td.date") else "")

        pm = re.search(r"\[([0-9]+ì›”\s*[0-9]+ì£¼ì°¨)\]", list_title)
        period_label = pm.group(1) if pm else list_title

        out.append({
            "no": no, 
            "period_label": period_label, 
            "list_date": list_date
        })
    return out

# crawl_from_list: ì—¬ëŸ¬ ëª©ë¡ í˜ì´ì§€ë¥¼ ëŒë©° ì£¼ê°„ ê¸°ì‚¬ë“¤ì„ ëª¨ë‘ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
def crawl_from_list(pages: int = 1, limit: int | None = None, use_external_fallback: bool = False) -> list[dict]:
    """
    ëª©ë¡ í˜ì´ì§€ë¥¼ ì•ì—ì„œë¶€í„° `pages` ì¥ í›‘ì–´ ìµœì‹  ê¸€ë“¤ì˜ ìƒì„¸ë¥¼ í¬ë¡¤ë§.
    - pages: 1ì´ë©´ ì²« í˜ì´ì§€(ìµœì‹  10ê°œ)ë§Œ
    - limit: ì´ ìˆ˜ì§‘ ìƒí•œ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
    """
    # KISTIëŠ” ë³´í†µ pageIndex íŒŒë¼ë¯¸í„°ë¡œ í˜ì´ì§• (ì—†ìœ¼ë©´ 1í˜ì´ì§€)
    all_list_items: list[dict] = []
    for page in range(1, pages + 1):
        html = fetch_html(LIST_URL, params={"pageIndex": page})
        items = parse_list_page(html)
        if not items:
            break
        all_list_items.extend(items)
        if limit and len(all_list_items) >= limit:
            all_list_items = all_list_items[:limit]
            break

    results: list[dict] = []
    seen_links: set[str] = set()
    for li in all_list_items:
        detail_url = BASE_DETAIL.format(no=li["no"])
        detail_items = parse_detail(detail_url, use_external_fallback=True)
        # í•´ë‹¹ ìƒì„¸ ê¸€ ì•ˆì— ì—¬ëŸ¬ ê¸°ì‚¬(title/link/summaryâ€¦)ê°€ ë“¤ì–´ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë³‘í•©
        for d in detail_items:
            period_label = li["period_label"]
            list_date = li.get("list_date", "")

            if not d.get("date") and list_date:
                if re.match(r"^\d{4}[.\-]\d{2}[.\-]\d{2}$", list_date):
                    d["date"] = list_date.replace(".", "-")

            week_key = derive_week_key(list_date, period_label)
            summary_text = truncate_summary(d.get("summary", ""))
            if not summary_text:
                summary_text = "ìš”ì•½ ì—†ìŒ"

            if d["link"] in seen_links:
                continue
            seen_links.add(d["link"])

            source_name = d.get("source") or DEFAULT_SOURCE

            category = categorize(d.get("title"), CATEGORY_DEFAULT)

            results.append({
                "id": sha1_hex(f"{DEFAULT_SOURCE}:{week_key}:{d['title']}:{d['link']}"),
                "title": d["title"],
                "link": d["link"],
                "summary": summary_text,
                "week": week_key,
                "date": d.get("date") or (list_date.replace(".", "-") if list_date else ""),
                "source": source_name,
                "category": category,
                "period_label": period_label,
                "thumbnail": d.get("thumbnail") or None,
            })
    return results


# ---------- CLI ----------

# print_usage: ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© ë°©ë²•ì„ ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´í•©ë‹ˆë‹¤.
def print_usage():
    print("Usage:")
    print("  python science_on_scraper.py list               # ìµœì‹  10ê°œ(1í˜ì´ì§€) ìƒì„¸ í¬ë¡¤ë§")
    print("  python science_on_scraper.py list 2             # 2í˜ì´ì§€(ìµœëŒ€ 20ê°œ) í¬ë¡¤ë§")
    print("  python science_on_scraper.py list 1 30          # 1í˜ì´ì§€ì—ì„œ ìµœëŒ€ 30ê°œ(ë„˜ì¹˜ë©´ ë‹¤ìŒ í˜ì´ì§€ë¡œ) í¬ë¡¤ë§")
    print("  python science_on_scraper.py 260                # ë‹¨ì¼ ìƒì„¸ í˜ì´ì§€ë§Œ (ê¸°ì¡´ ë™ì‘)")

# main: CLI ì§„ì…ì ìœ¼ë¡œ ëª©ë¡ ëª¨ë“œì™€ ë‹¨ì¼ ìƒì„¸ ëª¨ë“œë¥¼ ë¶„ê¸°í•©ë‹ˆë‹¤.
def main():
    # ê¸°ì¡´ ë‹¨ì¼ ìƒì„¸ ë™ì‘ ìœ ì§€ + ëª©ë¡ ëª¨ë“œ ì¶”ê°€
    if len(sys.argv) >= 2 and sys.argv[1].lower() == "list":
        pages = int(sys.argv[2]) if len(sys.argv) >= 3 else 1
        limit = int(sys.argv[3]) if len(sys.argv) >= 4 else None
        data = crawl_from_list(pages=pages, limit=limit, use_external_fallback=False)
        print(json.dumps(data, ensure_ascii=False, indent=2))
        return

    # ë‹¨ì¼ ìƒì„¸ (ê¸°ì¡´)
    no = sys.argv[1] if len(sys.argv) >= 2 else "260"
    url = BASE_DETAIL.format(no=no)
    data = parse_detail(url, use_external_fallback=False)
    print(json.dumps(data, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()
