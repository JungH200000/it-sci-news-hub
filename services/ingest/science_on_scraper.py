# -*- coding: utf-8 -*-
import re
import sys
import json
import requests
from bs4 import BeautifulSoup, Tag

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
BASE_DETAIL = "https://scienceon.kisti.re.kr/trendPromo/PORTrendPromoDtl.do?trendPromoNo={no}"

# ---------- text utils ----------
def clean_text(s: str) -> str:
    if not s: return ""
    s = s.replace("\xa0", " ").replace("\u00a0", " ")
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def looks_like_title(t: str) -> bool:
    if not t or len(t) < 4: return False
    if t.lower() in {"new", "(new)"}: return False
    return True

def smart_title_clean(s: str) -> str:
    # ë¶ˆë¦¿ ì´í›„/ë‚ ì§œ ê¼¬ë¦¬í‘œ ì œê±°
    s = clean_text(s)
    if "ã†" in s: s = s.split("ã†", 1)[0].strip()
    s = re.sub(r"\(\s*[^()]*\d{2,4}[.\-/]\d{1,2}([.\-/]\d{1,2})?[^()]*\)\s*$", "", s).strip()
    if len(s) > 180: s = s[:180].rstrip()
    return s

def strip_anchor_text(node: Tag) -> str:
    soup = BeautifulSoup(str(node), "lxml")
    for a in soup.find_all("a"): a.decompose()
    return soup.get_text(" ", strip=True)

def has_korean(t: str) -> bool:
    return bool(re.search(r"[ê°€-í£]", t or ""))

# ---------- page structure helpers ----------
def is_title_paragraph(p: Tag) -> bool:
    return (p.name == "p" and "MsoNormal" in (p.get("class") or []) and p.find("a", href=True) is not None)

def extract_link_from_p(p: Tag) -> str:
    for a in p.find_all("a", href=True):
        href = (a.get("href") or "").strip()
        if href.startswith("http"): return href
    return ""

def extract_title_from_p(p: Tag) -> str:
    # aê°€ ë“¤ì–´ìˆëŠ” ê°€ì¥ ê°€ê¹Œìš´ ì»¨í…Œì´ë„ˆ(span/strong/em) ìš°ì„ 
    a = p.find("a", href=True)
    if a:
        cur, depth = a.parent, 0
        while isinstance(cur, Tag) and depth < 5:
            if cur.name in {"span", "strong", "em"}:
                t = smart_title_clean(strip_anchor_text(cur))
                if looks_like_title(t): return t
            if cur == p: break
            cur = cur.parent; depth += 1
    # pì—ì„œ a ì œê±°
    t2 = smart_title_clean(strip_anchor_text(p))
    if looks_like_title(t2): return t2
    # ìµœí›„: p ì „ì²´
    title = smart_title_clean(p.get_text(" ", strip=True))
    # ì¹´í…Œê³ ë¦¬ í”„ë¦¬í”½ìŠ¤ ì œê±°
    title = strip_prefix_category(title)
    return title

def strip_prefix_category(title: str) -> str:
    """
    ì œëª© ë§¨ ì•ì˜ '- ( ... )' ë˜ëŠ” '( ... )' íŒ¨í„´ì„ ì œê±°.
    ì˜ˆ: '- ( ğŸ’¥ ì–‘ì ) ê±°ì‹œì„¸ê³„...' â†’ 'ê±°ì‹œì„¸ê³„...'
    """
    # ë§¨ ì•ì˜ ëŒ€ì‹œì™€ ê´„í˜¸ ë¸”ë¡ ì œê±°
    title = re.sub(r"^\s*-\s*\([^)]*\)\s*", "", title)
    title = re.sub(r"^\s*\([^)]*\)\s*", "", title)
    return title.strip()


# ---------- summary finder ----------
BULLET_START = re.compile(r"^[\s\u00A0]*[ã†Â·â€¢\-]+[\s\u00A0]*")

def split_first_bullet_line(text: str) -> str:
    """
    ê¸´ ë¬¸ë‹¨ì— ë¶ˆë¦¿ì´ ì—¬ëŸ¬ ê°œ ì´ì–´ì§ˆ ë•Œ, 'ì²« ë¶ˆë¦¿'ë§Œ í•œ ì¤„ ìš”ì•½ìœ¼ë¡œ.
    ì˜ˆ: 'ã† ... - ã† ...' í˜•íƒœ â†’ ì²« ë¶ˆë¦¿ ì¡°ê°ë§Œ.
    """
    # ë¨¼ì € ë§¨ ì• ë¶ˆë¦¿ ì œê±°
    t = BULLET_START.sub("", text).strip()
    # ë‹¤ìŒ ë¶ˆë¦¿/ëŒ€ì‹œ ì•ê¹Œì§€ ìë¥´ê¸°
    t = re.split(r"\s(?:[ã†Â·â€¢\-]\s)", t, maxsplit=1)[0]
    return clean_text(t)

def find_summary_between(root: Tag, start: Tag, end: Tag | None) -> str:
    """
    start(ì œëª© p) ì´í›„ë¶€í„° end(ë‹¤ìŒ ì œëª© p) ì´ì „ê¹Œì§€ ë²”ìœ„ë¥¼ í›‘ì–´ ìš”ì•½ í›„ë³´ë¥¼ ì°¾ëŠ”ë‹¤.
    ìš°ì„ ìˆœìœ„:
      1) ë¶ˆë¦¿ í¬í•¨ p (í•œê¸€ ìš°ì„ )
      2) ë§í¬ ì—†ëŠ” 14px ìŠ¤íƒ€ì¼ p (í•œê¸€ ìš°ì„ )
      3) ë§í¬ ì—†ëŠ” ì¼ë°˜ p (í•œê¸€ ìš°ì„ , 10~300ì)
    ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
    """
    # í›„ë³´ ë²„í‚·
    bullet_ko = None; bullet_any = None
    body14_ko = None; body14_any = None
    body_ko = None; body_any = None

    # next_elementsë¥¼ ì¨ì„œ h2 ê²½ê³„ë„ í†µê³¼
    for el in start.next_elements:
        if el is end: break
        if not isinstance(el, Tag): continue

        # ë‹¤ìŒ ì œëª© pë¥¼ ë§Œë‚˜ë©´ ì¢…ë£Œ
        if is_title_paragraph(el) and el is not start:
            break

        if el.name == "p" and "MsoNormal" in (el.get("class") or []):
            has_link = el.find("a", href=True) is not None
            raw = el.get_text(" ", strip=True)
            txt = clean_text(raw)

            # 1) ë¶ˆë¦¿ ì¼€ì´ìŠ¤
            if BULLET_START.search(txt):
                one = split_first_bullet_line(txt)
                if has_korean(one) and not bullet_ko: bullet_ko = one
                if not bullet_any: bullet_any = one
                continue

            # 2) 14px ë³¸ë¬¸ & ë§í¬ ì—†ìŒ
            if not has_link and "font-size: 14px" in (el.get("style") or ""):
                if has_korean(txt) and not body14_ko: body14_ko = txt
                if not body14_any: body14_any = txt
                continue

            # 3) ì¼ë°˜ ë³¸ë¬¸ & ë§í¬ ì—†ìŒ (ë„ˆë¬´ ê¸´ ê±´ ì œì™¸)
            if not has_link and 10 <= len(txt) <= 300:
                if has_korean(txt) and not body_ko: body_ko = txt
                if not body_any: body_any = txt

    return bullet_ko or body14_ko or body_ko or bullet_any or body14_any or body_any or ""

# ---------- optional external fallback ----------
def fetch_fallback_summary_from_link(link: str) -> str:
    try:
        rr = requests.get(link, headers=HEADERS, timeout=10)
        rr.raise_for_status()
    except Exception:
        return ""
    if not rr.encoding or rr.encoding.lower() in ("iso-8859-1", "us-ascii"):
        rr.encoding = rr.apparent_encoding
    ss = BeautifulSoup(rr.text, "lxml")
    for sel in ["meta[property='og:description'][content]", "meta[name='description'][content]"]:
        m = ss.select_one(sel)
        if m and m.get("content"):
            return clean_text(m["content"])
    return ""

## ---------- date and source ----------

def extract_date_and_source(raw_text: str) -> tuple[str, str]:
    """
    (ë™ì•„ì‚¬ì´ì–¸ìŠ¤ / 2025.09.18.) ê°™ì€ ê¼¬ë¦¬í‘œì—ì„œ sourceì™€ date ì¶”ì¶œ
    return (date, source) / ì—†ìœ¼ë©´ ("","")
    """
    m = re.search(r"\(([^()/]+)\s*/\s*(\d{2,4})[.\-](\d{1,2})[.\-](\d{1,2})", raw_text)
    if not m:
        return "", ""
    source = m.group(1).strip()
    yyyy, mm, dd = int(m.group(2)), int(m.group(3)), int(m.group(4))
    # ì—°ë„ ë³´ì •: 0~99 â†’ 2000+ì—°ë„ (ì˜ˆ: 25 â†’ 2025), 100~999 â†’ 2000+ì—°ë„ê°€ ê³¼í•˜ë©´ ìƒí™©ì— ë§ê²Œ ì¡°ì •
    if yyyy < 100:
        yyyy += 2000
    elif 100 <= yyyy < 1000:
        # ëŒ€ë¶€ë¶„ ì˜¤íƒˆì(ì˜ˆ: 025) ì¼€ì´ìŠ¤ë§Œ ì¡´ì¬í•˜ë¯€ë¡œ 100~999ëŠ” ë“œë¬¼ë‹¤.
        # í•„ìš”í•˜ë©´ ê·œì¹™ í™•ì¥ ê°€ëŠ¥. ì¼ë‹¨ 2000ëŒ€ê°€ ì•„ë‹ˆë©´ 2000 ë”í•˜ì§€ ì•ŠìŒ.
        pass

    date_str = f"{yyyy:04d}-{mm:02d}-{dd:02d}"
    return date_str, source

# ë²”ìœ„ì—ì„œ (source / yyyy.mm.dd) ê¼¬ë¦¬í‘œë¥¼ ì°¾ì•„ì„œ ë°˜í™˜
DATE_SOURCE_PAT = re.compile(
    r"[ï¼ˆ(]\s*([^()ï¼ˆï¼‰/|]+?)\s*[/|]\s*(\d{2,4})[.\-](\d{1,2})[.\-](\d{1,2})\.?\s*[)ï¼‰]"
)

def extract_date_source_near(root: Tag, start: Tag, end: Tag | None) -> tuple[str, str]:
    """
    start(ì œëª© p) ì´í›„ ~ end(ë‹¤ìŒ ì œëª© p) ì´ì „ ë²”ìœ„ì—ì„œ
    '(ì¶œì²˜ / YYYY.MM.DD.)' íŒ¨í„´ì„ ì°¾ì•„ (date, source) ë°˜í™˜.
    ëª» ì°¾ìœ¼ë©´ ("","")
    """
    for el in start.next_elements:
        if el is end:
            break
        if not isinstance(el, Tag):
            continue
        if is_title_paragraph(el) and el is not start:
            break

        # í…ìŠ¤íŠ¸ ë§ì€ p/spanì—ë§Œ ì‹œë„
        if el.name in {"p", "span"}:
            raw = el.get_text(" ", strip=True)
            m = DATE_SOURCE_PAT.search(raw)
            if m:
                source = m.group(1).strip()
                yyyy, mm, dd = int(m.group(2)), int(m.group(3)), int(m.group(4))
                date_str = f"{yyyy:04d}-{mm:02d}-{dd:02d}"  # KST ë²„í‚· YYYY-MM-DD
                return date_str, source
    return "", ""


# ---------- main parse ----------
def parse_detail(detail_url: str, use_external_fallback: bool = False):
    r = requests.get(detail_url, headers=HEADERS, timeout=30)
    r.raise_for_status()
    if not r.encoding or r.encoding.lower() in ("iso-8859-1", "us-ascii"):
        r.encoding = r.apparent_encoding

    soup = BeautifulSoup(r.text, "lxml")
    root = soup.select_one("div.board-view-content")
    if not root: raise RuntimeError("board-view-content ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # 1) ì œëª© p ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
    title_ps: list[Tag] = [p for p in root.find_all("p", class_="MsoNormal") if is_title_paragraph(p)]

    items = []
    for i, p in enumerate(title_ps):
        link = extract_link_from_p(p)
        if not link.startswith("http"): continue

        # ì œëª© ì›ë¬¸ í…ìŠ¤íŠ¸
        title_raw = p.get_text(" ", strip=True)

        # ë‚ ì§œ + ì¶œì²˜ ê¼¬ë¦¬í‘œì—ì„œ ì¶”ì¶œ
        date, source = extract_date_and_source(title_raw)

        # ì œëª© ì •ë¦¬
        title = smart_title_clean(title_raw)
        title = strip_prefix_category(title)

        if not looks_like_title(title): continue

        # 2) ë‹¤ìŒ ì œëª© pë¥¼ ê²½ê³„ë¡œ ìš”ì•½ íƒìƒ‰
        end = title_ps[i+1] if i+1 < len(title_ps) else None
        summary = find_summary_between(root, p, end)

        # ğŸ”¹ ìš”ì•½ ë²”ìœ„ì—ì„œ ë‚ ì§œ/ì¶œì²˜ ìŠ¤ìº” (ì œëª© p ì•ˆì— ì—†ì„ ë•Œ ëŒ€ë¹„)
        date, source = extract_date_source_near(root, p, end)

        # ê·¸ë˜ë„ ëª» ì°¾ì•˜ê³ , ì œëª© rawì— ìˆëŠ” ê²½ìš°ì—” ë³´ì¡° ì¶”ì¶œ (ê¸°ì¡´ í•¨ìˆ˜ ê·¸ëŒ€ë¡œ í™œìš© ê°€ëŠ¥)
        if not date or not source:
            title_raw = p.get_text(" ", strip=True)
            d2, s2 = extract_date_and_source(title_raw)
            if d2 and s2:
                date, source = d2, s2

        # 3) ì •ë§ ë¹„ë©´(ê·¸ë¦¬ê³  ì›í•  ë•Œë§Œ) ì™¸ë¶€ ë©”íƒ€ fallback
        if not summary and use_external_fallback:
            summary = fetch_fallback_summary_from_link(link)
        
        items.append({
            "title": title,
            "link": link,
            "summary": summary,
            "week": date,
            "source": source
        })

    return items

# ---------- list page (ëª©ë¡) -> detail nos ----------

LIST_URL = "https://scienceon.kisti.re.kr/trendPromo/PORTrendPromoList.do"

JS_NO_RE = re.compile(r"fn_moveTrendPromoDtl\('(\d+)'\)")

def fetch_html(url: str, params: dict | None = None) -> str:
    r = requests.get(url, params=params or {}, headers=HEADERS, timeout=30)
    r.raise_for_status()
    if not r.encoding or r.encoding.lower() in ("iso-8859-1", "us-ascii"):
        r.encoding = r.apparent_encoding
    return r.text

def parse_list_page(html: str) -> list[dict]:
    """
    ëª©ë¡ í˜ì´ì§€ì—ì„œ ìµœì‹  ê²Œì‹œê¸€ë“¤ì˜ trendPromoNo, ë¦¬ìŠ¤íŠ¸ íƒ€ì´í‹€, ë¦¬ìŠ¤íŠ¸ ë‚ ì§œ ì¶”ì¶œ
    return: [{"no":"260","list_title":"[9ì›” 4ì£¼ì°¨] ê¸ˆì£¼ì˜ ê³¼í•™ê¸°ìˆ ë‰´ìŠ¤(1)","list_date":"2025-09-23"}, ...]
    """
    soup = BeautifulSoup(html, "lxml")
    rows = soup.select("table.board-list-tbl tbody tr")
    out = []
    for tr in rows:
        a = tr.select_one("td.subject a[href]")
        if not a:
            continue
        m = JS_NO_RE.search(a.get("href", ""))
        if not m:
            continue
        no = m.group(1)
        list_title = clean_text(a.get_text(" ", strip=True))
        list_date = clean_text((tr.select_one("td.date") or {}).get_text(" ", strip=True) if tr.select_one("td.date") else "")

        pm = re.search(r"\[([0-9]+ì›”\s*[0-9]+ì£¼ì°¨)\]", list_title)
        period_label = pm.group(1) if pm else list_title

        out.append({
            "no": no, 
            "period_label": period_label, 
            "list_date": list_date
        })
    return out

def crawl_from_list(pages: int = 1, limit: int | None = None, use_external_fallback: bool = False) -> list[dict]:
    """
    ëª©ë¡ í˜ì´ì§€ë¥¼ ì•ì—ì„œë¶€í„° `pages` ì¥ í›‘ì–´ ìµœì‹  ê¸€ë“¤ì˜ ìƒì„¸ë¥¼ í¬ë¡¤ë§.
    - pages: 1ì´ë©´ ì²« í˜ì´ì§€(ìµœì‹  10ê°œ)ë§Œ
    - limit: ì´ ìˆ˜ì§‘ ìƒí•œ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
    """
    # KISTIëŠ” ë³´í†µ pageIndex íŒŒë¼ë¯¸í„°ë¡œ í˜ì´ì§• (ì—†ìœ¼ë©´ 1í˜ì´ì§€)
    all_list_items: list[dict] = []
    for page in range(1, pages + 1):
        html = fetch_html(LIST_URL, params={"pageIndex": page})
        items = parse_list_page(html)
        if not items:
            break
        all_list_items.extend(items)
        if limit and len(all_list_items) >= limit:
            all_list_items = all_list_items[:limit]
            break

    results: list[dict] = []
    for li in all_list_items:
        detail_url = BASE_DETAIL.format(no=li["no"])
        detail_items = parse_detail(detail_url, use_external_fallback=use_external_fallback)
        # í•´ë‹¹ ìƒì„¸ ê¸€ ì•ˆì— ì—¬ëŸ¬ ê¸°ì‚¬(title/link/summaryâ€¦)ê°€ ë“¤ì–´ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë³‘í•©
        for d in detail_items:
            # ìƒì„¸ì—ì„œ date/sourceë¥¼ ëª» ì¡ì•˜ìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ ë‚ ì§œë¥¼ ë³´ì¡°ë¡œ ì‚¬ìš© (YYYY-MM-DD ë˜ëŠ” YYYY.MM.DD ë‘˜ ë‹¤ ì»¤ë²„)
            if not d.get("date"):
                # list_dateê°€ 'YYYY-MM-DD' ë˜ëŠ” 'YYYY.MM.DD'ì¼ ìˆ˜ ìˆìŒ â†’ '-' í¬ë§·ìœ¼ë¡œ ë³´ì •
                ld = li.get("list_date", "")
                if re.match(r"^\d{4}[.\-]\d{2}[.\-]\d{2}$", ld):
                    d["date"] = ld.replace(".", "-")
            # ì›í•˜ë©´ ëª©ë¡ ì œëª©ë„ ë©”íƒ€ë¡œ ë‚¨ê²¨ë‘ê¸°
            d.setdefault("period_label", li["period_label"])
            d.setdefault("trendPromoNo", li["no"])
            results.append(d)
    return results


# ---------- CLI ----------

def print_usage():
    print("Usage:")
    print("  python science_on_scraper.py list               # ìµœì‹  10ê°œ(1í˜ì´ì§€) ìƒì„¸ í¬ë¡¤ë§")
    print("  python science_on_scraper.py list 2             # 2í˜ì´ì§€(ìµœëŒ€ 20ê°œ) í¬ë¡¤ë§")
    print("  python science_on_scraper.py list 1 30          # 1í˜ì´ì§€ì—ì„œ ìµœëŒ€ 30ê°œ(ë„˜ì¹˜ë©´ ë‹¤ìŒ í˜ì´ì§€ë¡œ) í¬ë¡¤ë§")
    print("  python science_on_scraper.py 260                # ë‹¨ì¼ ìƒì„¸ í˜ì´ì§€ë§Œ (ê¸°ì¡´ ë™ì‘)")

def main():
    # ê¸°ì¡´ ë‹¨ì¼ ìƒì„¸ ë™ì‘ ìœ ì§€ + ëª©ë¡ ëª¨ë“œ ì¶”ê°€
    if len(sys.argv) >= 2 and sys.argv[1].lower() == "list":
        pages = int(sys.argv[2]) if len(sys.argv) >= 3 else 1
        limit = int(sys.argv[3]) if len(sys.argv) >= 4 else None
        data = crawl_from_list(pages=pages, limit=limit, use_external_fallback=False)
        print(json.dumps(data, ensure_ascii=False, indent=2))
        return

    # ë‹¨ì¼ ìƒì„¸ (ê¸°ì¡´)
    no = sys.argv[1] if len(sys.argv) >= 2 else "260"
    url = BASE_DETAIL.format(no=no)
    data = parse_detail(url, use_external_fallback=False)
    print(json.dumps(data, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()

